{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os.path import basename, join, splitext\n",
    "from os import chdir\n",
    "from glob import glob\n",
    "\n",
    "INPUT_BASE_DIR = 'anserini_runs'\n",
    "OUTPUT_DIR = join('analyses', 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing: anserini_runs/single/should/bdi_optext/eRisk18/1000/*\n"
     ]
    }
   ],
   "source": [
    "# This process should be executed process for each combination of collection,\n",
    "# query formulation strategy and number of hits (top-k) #\n",
    "corpus_suffix = ['eRisk18', 'eRisk19', 'clpsych15', 'clpsych15_ctrl']\n",
    "bdi_suffix = ['bdi_optext']\n",
    "hits_suffix = ['10', '1000']\n",
    "\n",
    "# -- Anserini result file -- #\n",
    "# Query Result File Format:\n",
    "# * 1st column is the topic number (i.e., query ID)\n",
    "# * 2nd column is currently unused and should always be \"Q0\".\n",
    "# * 3rd column is the official document identifier of the retrieved document.\n",
    "# * 4th column is the rank the document is retrieved.\n",
    "# * 5th column shows the score (integer or floating point) that generated the ranking.\n",
    "# * 6th column is called the \"run tag\" and should be a unique identifier for your system.\n",
    "# Separated by a single **space**\n",
    "# 1 Q0 413516 1 5.086600 Anserini #\n",
    "trec_eval_fmt_cols = ['qid', 'unused', 'docid', 'rank', 'score', 'runtag']\n",
    "\n",
    "# Retrieval Models used #\n",
    "retrieval_models = ['bm25', 'qld', 'bm25+rm3', 'qld+rm3', 'bm25+bm25prf']\n",
    "\n",
    "operator = 'should'\n",
    "grain = 'single'\n",
    "\n",
    "# Select the corresponding index based on the desired analysis #\n",
    "corpus = corpus_suffix[0]\n",
    "bdi = bdi_suffix[1]\n",
    "hits = hits_suffix[2]\n",
    "\n",
    "path = join(INPUT_BASE_DIR, grain, operator, bdi, corpus, hits, '*')\n",
    "print('Listing: %s' % path)\n",
    "\n",
    "ls = sorted(glob(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: anserini_runs/single/should/bdi_optext/eRisk18/1000/bm25+bm25prf_OR_1000_optext-bdi.txt\n",
      "Processing: anserini_runs/single/should/bdi_optext/eRisk18/1000/bm25+rm3_OR_1000_optext-bdi.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: anserini_runs/single/should/bdi_optext/eRisk18/1000/bm25_OR_1000_optext-bdi.txt\n",
      "Processing: anserini_runs/single/should/bdi_optext/eRisk18/1000/qld+rm3_OR_1000_optext-bdi.txt\n",
      "Processing: anserini_runs/single/should/bdi_optext/eRisk18/1000/qld_OR_1000_optext-bdi.txt\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of queries #\n",
    "# One query per BDI item (Total 21) #\n",
    "max_qid = 22\n",
    "retrieval_prf_mean = {}\n",
    "result_set_size = {}\n",
    "\n",
    "# **Incidence Score**: Compute the average relevance score of the documents \n",
    "# retrieved for each BDI item based on a specific ranking function #\n",
    "# Thus, each row of _retrieval_prf_mean_ contains the average score of item \"i\", \n",
    "# for different ranking functions #\n",
    "for filename in ls:\n",
    "    print('Processing: %s' % filename)\n",
    "    # Parse filename #\n",
    "    rkg_mdl, _operator, _hits, _bdi = splitext(basename(filename))[0].split('_')\n",
    "    # Read result file (ssv) #\n",
    "    ranking = pd.read_csv(filename, header=None, sep=' ')\n",
    "    ranking.columns = trec_eval_fmt_cols\n",
    "    ranking.drop(['unused', 'runtag'], axis=1, inplace=True)\n",
    "    q_mean = np.zeros(max_qid, dtype=np.float64)\n",
    "    q_result_set_size = np.zeros(max_qid, dtype=np.int32)\n",
    "    for i in range(1, max_qid):\n",
    "        q = ranking.loc[ranking.qid == i]\n",
    "        q_mean[i] = q.score.mean()\n",
    "        q_result_set_size[i] = q.shape[0]\n",
    "    retrieval_prf_mean[rkg_mdl] = q_mean\n",
    "    result_set_size[rkg_mdl] = q_result_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = [join(OUTPUT_DIR, 'retrieval_prf_mean_%s_%s_%s.tsv' % (operator, hits, bdi)),\n",
    "            join(OUTPUT_DIR, 'result_set_size_%s_%s_%s.tsv' % (operator, hits, bdi))]\n",
    "pd.DataFrame.from_dict(retrieval_prf_mean).drop(index=0).to_csv(filepath[0], \n",
    "                                                                index=True, \n",
    "                                                                header=True, \n",
    "                                                                sep='\\t')\n",
    "pd.DataFrame.from_dict(result_set_size).drop(index=0).to_csv(filepath[1], \n",
    "                                                             index=True, \n",
    "                                                             header=True, \n",
    "                                                             sep='\\t')\n",
    "retrieval_prf = pd.read_csv(filepath[0], header=0, index_col=0, sep='\\t')\n",
    "result_set_size = pd.read_csv(filepath[1], header=0, index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on these incidence scores, sort the BDI items creating an ordered list \n",
    "# of the 21 elements.\n",
    "rankings = np.zeros((max_qid - 1, len(retrieval_models)), dtype=np.int32)\n",
    "hits_by_query = np.zeros((max_qid - 1, len(retrieval_models)), dtype=np.int32)\n",
    "for idx, rm in enumerate(retrieval_models):\n",
    "    # For each ranking function (column in _retrieval_prf_), sort the row based\n",
    "    # on the incidence score. Keep the index of the row it represents the item\n",
    "    # index.\n",
    "    sorted_idxs = retrieval_prf.loc[:, rm].sort_values(ascending=False).index\n",
    "    rankings[:, idx] = sorted_idxs.values\n",
    "    hits_by_query[:, idx] = result_set_size.loc[sorted_idxs, rm]\n",
    "questions_ranking = pd.DataFrame(rankings, columns=retrieval_models)\n",
    "paired_result_set_size = pd.DataFrame(hits_by_query, columns=retrieval_models)\n",
    "questions_ranking.to_csv(join(OUTPUT_DIR, 'ranking.tsv'), header=True, sep='\\t')\n",
    "paired_result_set_size.to_csv(join(OUTPUT_DIR, 'hits.tsv'), header=True, sep='\\t')\n",
    "\n",
    "# BDI Items are now sorted based on the incidence score, thus at row zero you\n",
    "# can find the BDI item with the highest incidence (item's index). Equivalently, \n",
    "# at row twenty, you can find the item with the lowest incidence #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
